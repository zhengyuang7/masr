![image](https://github.com/user-attachments/assets/57dccffc-2093-4df3-abbd-067bef9f019b)![image](https://github.com/user-attachments/assets/bc5a2850-1f79-467c-97ff-b7d6e4ba9628)![image](https://github.com/user-attachments/assets/1e27c1de-f433-43d8-aca4-de42d73fc0a2)ssl
- MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research
- Enhancing Multilingual Speech Recognition through Language Prompt Tuning and Frame-Level Language Adapter
- Anatomy of Industrial Scale Multilingual ASR


moe
- Switch Transformers: Scaling to Trillion Parameter Models with Simple and Ecient Sparsity
- Mixture-of-Expert Conformer for Streaming Multilingual ASR
- OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER
- Wav2vec-MoE: An unsupervised pre-training and adaptation method for multi-accent ASR
- Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition
- BLR-MoE: Boosted Language-Routing Mixture of Experts for Domain-Robust Multilingual E2E ASR


extend language
- EXTENDING MULTILINGUAL ASR TO NEW LANGUAGES USING SUPPLEMENTARY ENCODER AND DECODER COMPONENTS

